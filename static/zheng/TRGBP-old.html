<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,minimum-scale=1,maximum-scale=1,user-scalable=no">
    <meta name="format-detection" content="telephone=no,email=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="robots" content="index,follow">
    <meta name="referrer" content="never">
    <title>BJTU-ADAM Lab - 北京交通大学ADAM人工智能实验室官网</title>
    <link href="https://ai.tencent.com/ailab/images/favicon.ico" type="image/ico" rel="shortcut icon">
    <script src="./zheng_files/segment.js"></script>
    <link href="./zheng_files/app.b56908a4ef7f7c7df6627f70cc26fc81.css" rel="stylesheet">
    <script type="text/javascript" charset="utf-8" async="" src="./zheng_files/9.691b19c55c2987dd0cf3.js"></script>
</head>

<body _c_t_common="1" style="zoom: 1;" _c_t_j1="1" data-new-gr-c-s-check-loaded="14.1033.0" data-gr-ext-installed="">
<div class="wrap wrap2 wrap_detial">
    <header class="header whiteBg">
        <div class="inner">
            <div class="lang"><a href="https://adam-bjtu.org"
                                 class="router-link-exact-active router-link-active" id="zh" aria-current="page">中</a>
                <span>|</span> <a href="https://adam-bjtu.org" class="langUnFocus"
                                  id="en">En</a></div>
            <div class="navbar" >
                <ul style="height: 100%;">
                    <li class="paperNav"><a href="../../index.html"
                                            class="router-link-exact-active router-link-active"><span>主页</span></a></li>
                    <!-- <li class="paperNav"><a href="adam-bjtu.org/paper/huang"
                            class="router-link-exact-active router-link-active"><span>论文</span></a></li>
                    <li class="aboutNav"><a href="/about"
                            class=""><span>关于</span></a></li> -->
                    <li class="lang lang2"><a id="zh" class="">中</a> <span></span> <a id="en"
                                                                                      class="langUnFocus">En</a></li>
                </ul>
            </div>
            <!----> <a href="javascript:;" class="nav_btn"></a>
        </div>
    </header>

    <br><br><br><br><br>
    <table border="0" width="1000px" align="center">
        <tbody>
        <tr>
            <td>

                <center>
                    <h1>
                        <font face="helvetica" style="font-size:87%">
                            Towards Predictable Feature Attribution: 
                            <br>
                            Revisiting and Improving Guided BackPropagation 
                        </font>
                    </h1>
                </center>

                <br>
                <div>
                    <a href="" style="text-decoration: none;">
                        <img src="./TRGBP/download_button.jpg" height="30px">
                    </a>
                    <div style="margin-left: 10px; margin-top: 20px; display: inline-block;">
                        <a href="https://openreview.net/forum?id=uXpTNpkXFLB">Paper Available</a>
                    </div>
                </div>
                <h2><font face="helvetica" style="font-size:24px">Introduction</font></h2>
                <hr style="margin-top:-10px; margin-bottom:13px">
                <font face="helvetica" style="font-size:18px">
                    <p align="justify">
                        Recently, backpropagation(BP)-based feature attribution methods have been widely adopted to interpret the 
                        internal mechanisms of convolutional neural networks (CNNs), and expected to be human-understandable (lucidity) 
                        and faithful to decision-making processes (fidelity). In this paper, we introduce a novel property for feature 
                        attribution: predictability, which means users can forecast behaviors of the interpretation methods. With the 
                        evidence that many attribution methods have unexpected and harmful phenomena like class-insensitivity, the 
                        predictability is critical to avoid over-trust and misuse from users. Observing that many intuitive improvements 
                        for lucidity and fidelity tend to sacrifice predictability, we propose a new visual explanation method called 
                        TR-GBP (Theoretical Refinements of Guided BackPropagation) which revisits and improves GBP from theoretical 
                        perspective rather than solely optimizing the attribution performance. Qualitative and quantitative experiments 
                        show that TR-GBP is more visually sharpened, gets rid of the fidelity problems in GBP, and effectively predicts 
                        the possible behaviors so that we can easily discriminate some prediction errors from interpretation errors.
                    </p>

                    <p>
                        <b>Our main contributions can be summarized three-fold:</b>
                    </p>
                    <ol style="margin-top:-7px">
                        <li>
                            In addition to lucidity and fidelity, we introduce a novel and critical property predictability 
                            for BP-based feature attributions. 
                        </li>
                        <li>
                            We propose a new predictable feature attribution method TR-GBP, by addressing two fundamental 
                            issues in the reconstruction theory of GBP. 
                        </li>
                        <li>
                            Qualitative and quantitative experiments are performed and show that TR-GBP can obtain sharper visualizations, 
                            get rid of the fidelity problems of GBP, and enables us to discriminate some prediction errors from 
                            interpretation errors.
                        </li>
                    </ol>

                </font>

                <p style="padding-bottom:1px"></p>


                <h2><font face="helvetica" style="font-size:24px">Motivation</font></h2>
                <hr style="margin-top:-10px; margin-bottom:13px">
                <font face="helvetica" style="font-size:20px">


                    <h3>Predictability means that humans can understand and foresee the outcome of an execution, 
                        for example, over three metres high, and one would be afraid to jump down, even if one has
                         never done it before. Considering that it would be too late if one did jump directly from 
                         a high building, we cannot always mend the problem after it has arisen. Reliable and trustworthy 
                         gradient attribution methods therefore require predictability. However, existing ideas for
                         improvement sacrifice predictability, so improved methods require a supporting theoretical 
                         explanation. Here we will improve the reconstruction theory that accompanies the GBP in order 
                         to achieve improvements to the GBP.</h3>
                    <br>
                    <p style="padding-bottom:1px"></p>


                    <h2><font face="helvetica" style="font-size:24px">Reconstruction theory of GBP</font></h2>
                    <hr style="margin-top:-10px; margin-bottom:13px">
                    <font face="helvetica" style="font-size:20px">
                        <h3>As our proposed method is based on the reconstruction theory (Nie et al., 2018), 
                            we introduce its main conclusions before further discussions.</h3>
                        <center>
                            <img src="./TRGBP/Theorem1.png" width="950">
                        </center>
                        <center>
                            <img src="./TRGBP/Proposition1.png" width="950">
                        </center>
                        <br>
                    </font>

                    <h2><font face="helvetica" style="font-size:24px">Addressing the Limitations of Reconstruction Theory</font>
                    </h2>
                    <hr style="margin-top:-10px; margin-bottom:13px">
                    <font face="helvetica" style="font-size:20px">
                        <center>
                            <img src="./TRGBP/pipeline.png" width="1000">
                            <br>
                            <strong>Fig.1</strong>  Pipeline of TR-GBP. The forward propagation treats the bias as input maps fullfilled with
                            1, and the back propagation procedure also attribute to the middle inputs. Then we aggregate these
                            middle input with upsampling, and minus the mean values of top5 results.
                        </center>
                        <h3>We find two limitations:</h3>
                        <h3><strong>(1) The missing of bias terms:</strong> as reconstruction theory does not take the widely adopted 
                            bias terms into consideration, its conclusions cannot be directly applied to traditional 
                            networks. As bias terms also contribute to decision-making processes, the missing of bias 
                            terms will make attribution results incomplete and ignore the high-layer changes. That 
                            is why GBP failed the sanity checks (Adebayo et al., 2018). </h3>
                        <br>
                        <h3><strong>Our solution:</strong></h3>
                        <center>
                            <img src="./TRGBP/treatment1.png" width="950">
                        </center>
                        <h3><strong>(2) The similarity of V:</strong> according to the central limited theorem and the Proposition 1, 
                            the backpropagation results might have similar distributions for different classes, and so that GBP 
                            is class-insensitive. </h3>
                        <br>
                        <h3><strong>Our solution:</strong></h3>
                        <center>
                            <img src="./TRGBP/treatment2.png" width="950">
                        </center>
                        
                    </font>
                    <p style="padding-bottom:1px"></p>

                    <h2><font face="helvetica" style="font-size:24px">Experimental results</font></h2>
                    <hr style="margin-top:-10px; margin-bottom:13px">
                    <h2><font face="helvetica" style="font-size:24px">Lucidity</font></h2>
                    <font face="helvetica" style="font-size:20px">
                        <h3>We perform qualitative visual evaluation for TR-GBP along with baselines on validation 
                            set of ImageNet: saliency, GBP, GIG, GradCAM, FullGrad, CAMERAS. These methods are the 
                            newest or the most classical attribution methods of three kinds of attributions. 
                            Furthermore, we use the commonly used pretrained models: VGG16 and ResNet50 from torchvision 
                            model zoo. The results are shown in Figs.2, it can be seen that saliency is full of noise, 
                            GBP and GIG highlight the edges and FullGrad, GradCAM, TR-GBP shed light on a complete region. 
                            This is not surprising that TR-GBP is more complete than GBP and more tightly confined to object 
                            regions than GradCAM, FullGrad, CAMERAS, because we supplement the bias attributions for GBP and 
                            theoretical guarantee low noise level</h3>
                            <center>
                                <img src="./TRGBP/lucidity.png" width="1000">
                                <br>
                                <strong>Fig.2</strong>  Visualization results on VGG16 and ResNet50 of 
                                saliency, GBP, GIG, FullGrad, GradCAM, CAMERAS and our method TR-GBP.
                            </center>
                        <br>
                    </font>
                    <h2><font face="helvetica" style="font-size:24px">Fidelity</font></h2>
                    <font face="helvetica" style="font-size:20px">
                        <h3>We solve two fidelity problem of GBP:</h3>
                        <h3><strong>(1) Class-insensitivity:</strong>Figs. 3 shows our results on cat and dog image for ResNet50. 
                            The top1 class of output is ’bull mastiff’ and the top2 class is ’tiger cat’. It is evident 
                            that our method can distinguish different classes, and provide entire objects 
                            except for the tail of the cat. Such results are reasonable as we remove the 
                            reconstruction shared by different categories.</h3>
                            <center>
                                <img src="./TRGBP/fidelity1.png" width="800">
                                <br>
                                <strong>Fig.3</strong>  Class discriminative results for TRGBP. The middle heatmap is obtained from the
                                class ‘bull mastiff’, and the right heatmap is obtained from the class ‘tiger cat’.
                            </center>
                        <br>
                        <h3><strong>(2) Sanity checks:</strong>Adebayo et al. (2018) point out that some attribution methods are 
                            not able to show the differences between different models, just like special edge detectors. 
                            Specifically, they randomize some layers of model, and find that some attribution results, 
                            especially GBP, almost remain unchanged. We also perform a sanity check of TR-GBP and present 
                            the results in Figs. 4. As can be seen, our method is sensitive to model parameters and can 
                            efficiently reflect the differences between models before and after randomization. Moreover, 
                            the attribution results of TR-GPB converge to uniform with weak boundaries and random noises.</h3>
                            <center>
                                <img src="./TRGBP/fidelity2.png" width="1000">
                                <br>
                                <strong>Fig.4</strong> Sanity checks for different layers on ResNet50. From left to right, it reflect the cascade
                                randomization of model.
                            </center>
                    </font>
                    <h2><font face="helvetica" style="font-size:24px">Predictability</font></h2>
                    <font face="helvetica" style="font-size:20px">
                        <h3>Let us look closely at the results of Figs.4, the changes of fc layer lead to high scores in 
                            boundaries. As we have predicted that the boundaries of attribution must be weaker, 
                            if there are high scores in boundary pixels of TR-GBP, it must originate from model 
                            prediction errors. Note that such boundary dependencies are also shown by previous methods, 
                            like CAMERAS in Figs.4. Therefore, we attempt to conduct validation experiments to make sure 
                            such boundary dependencies in our method are actually a prediction errors but others not. 
                            Specifically, we use a metric which is similar to EBPG to represent the intensities of boundary 
                            dependencies (IBD):</h3>
                            <center>
                                <img src="./TRGBP/IBD.png" width="500">
                                <br>
                            </center>
                            <h3>where boundary is the 16-pixel boundary regions: height, width &lt 16 or height, width &gt 224-16. 
                                We discard half data in ImageNet validation set with the minimal IBD to obtain attribution 
                                results with salient boundary dependencies. Note that the dependency originates from 
                                prediction errors must have capacities to show the correctness of predictions, so we use 
                                traditional AUC (Area Under Curve) values of ROC curve to measure whether IBD can be used 
                                as a valid indicator to judge the correctness of model predictions. The results are shown 
                                in Table 1. TR-GBP has remarkable advance performance, which means that the boundary 
                                dependencies in TR-GBP are actually derived from prediction errors while other methods not, 
                                so TR-GBP succeeds in disentangling prediction errors from interpretation errors with the 
                                help of predictability.</h3>
                                <br>
                            <center>
                                <img src="./TRGBP/predictability.png" width="1000">
                                <br>
                                <strong>Table 1</strong> Comparative evaluation on AUC of IBD (higher is better).

                            </center>
                        <br>
                    </font>

                    <h2><font face="helvetica" style="font-size:24px">Reference</font></h2>
                    <hr style="margin-top:-10px; margin-bottom:13px">


                    <font face="helvetica" style="font-size:15px">

                       <h2>Weili Nie, Yang Zhang, and Ankit Patel. A theoretical explanation 
                           for perplexing behaviors of backpropagation-based visualizations. In International 
                           Conference on Machine Learning, pp. 3809–3818. PMLR, 2018.</h2>
                        <h2>Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and 
                            Been Kim. Sanity checks for saliency maps. arXiv preprint arXiv:1810.03292, 2018.</h2>
                        <br>
                    </font></font></td>
        </tr>
        </tbody>
    </table>

    <div class="clear"></div>
    <div class="footer">
        <div class="footer-box">
            <!-- <div class="left"><img
                    src="data:image/png;base64,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"
                    alt=""></div> -->
            <p>Copyright © 2021 ADaM. All Rights Reserved.</p>
            <ul class="right">
                <!-- <li class="footer_wechat"><a href="javascript:void(0);"><img
                            src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAkCAMAAADFCSheAAAAeFBMVEUAAABfY2hfZGlgZGpgY2lgY2hhZWtjam1gY2lgZGlgZGlkZmt2dopgY2hgZGlfZGlkZWlsbG9gY2lfY2hgZGhfZGlfZGhkaG9gZGhgZGhhZGlgZWlhZWpgZGlhZGhhZGtmd3dgZGlgZGhgY2lgZGlhamxgY2lfY2j4pgKsAAAAJ3RSTlMA+vVI4s84GYl0WSME13lkKgzryr2gjxOYfl9UMJJpUAfFrqSEHrTAcFJPAAABdElEQVQ4y42T6baCMAyE05Z9EWQXRHHN+7/hBWIVWqr3+zWnzAlpJ4EPblvmFueWXSYhfEcUuKDoIjBytFHBPoIBn6EGO8AmDW5SbrVSoYEKNM5oJACFvUUf7rXzUpfwcSWV7RXzgc7jUSYkU4AaiZ1ijuk4H2VA8gEwMJIWrHDwRQe1/X4FT56msKRFicWlyrK5scZvhau3rJPt3L3YVWV/CBb+fsvK/UEU7J18MsikUcdO03z9nzOZO92b7xOunjVz9KHmjesT6txgolCnzRG4hQeUxIqe8s9EeKXwUxoDNg14lK/NoS8LubNwAKit59z16jYXoBwbgOMszjILVs8Pwpatua/W721MwSY+Xw6sWNQ+OWhAjqDzcXfmXeiB+Cx38rPyMFVmsucfSybG3EQUBl75rMA2eFlNZo/7EUh8g/kKxMNVNliHEtQRhuttsz11JvR5rsCMsikYw1doByU8gu9M231r7kHoegwd+Ddp0cLIH4duiLZID23xAAAAAElFTkSuQmCC"></a>
                </li>
                <li class="footer_email"><a href="mailto:ailab@tencent.com"><img
                            src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAgCAMAAABemGpIAAAAdVBMVEUAAABgZGhqc4Bobm5fZGlgY2lfY2hhZWpjZ2xmaW5fY2hgY2lgZGlgZGlkZHNgZGlgZGlgZGlgY2hgZGlgZGlgZGlhZWtgY2lfZGhgY2lgZGliZ21jZWthaGtfZGhfZGlhZGhfZGlhY2lhZWhhZWpgZGlfY2gi7z+FAAAAJnRSTlMA+gcM8+vLQx0V3NeXhxDBvK2gj2JTNHnjs3EvKSbh0WlmXkw6erPR6wkAAAFwSURBVDjLhZKJsoIwDEVTbBFLURBUVveX///Ed1t3KHBmGAZ6oE1uCCRazKITenBXPIu6E4hWuOI5N46IVhEZ3RKt02k3XRO1R0Oa5ZIoMFOuCYiWkjUJZnEmolqMqaLG8tl6kEGOx5v0u/KGxZyBk0G1IGpKn1s2RIuKPzI42XLDoRvaZp34V2a1xx+yvpthx73ivsxyZ8P8dW1sO8kDGRywVHw9iwIvDsxembe2n+qdsO3/lv0y2CCpTrNDd0h2w+OyWNLLwHfYRkzIVwLY+3EicB2Xa/w2dFW5WkP0rR6TE/x0Y2+OBEcJcPPLBm71SOKVTwXb+OSMiLbPjJsmfDYTL7OhnAZ2tgbk2C7ty7aYP/aAUhfhrxxjuC7s5YJxjL/lY4SBEX5ZYMCi40dWXT+sfqydeslyj6GVY+5nHbL7slU8gWrtzpC1O1PJk5SuJk2mcNVO47pVGApcH2dBDlBdQvO4hMEqj8UscY6D/AMQB5GcWnbVDgAAAABJRU5ErkJggg=="></a>
                </li> -->
            </ul>
        </div>
    </div>
</div>
<script type="text/javascript" src="./zheng_files/manifest.a046e764a7f89ee70324.js"></script>
<script type="text/javascript" src="./zheng_files/vendor.bf6c914e1185a8f804a7.js"></script>
<script type="text/javascript" src="./zheng_files/app.fd4ed8c6867ac0aa97f6.js"></script>
<style>
    ol {
        counter-reset: sectioncounter;
        display: block;
        list-style-type: decimal;
        margin-block-start: 1em;
        margin-block-end: 1em;
        margin-inline-start: 0;
        margin-inline-end: 0;
        padding-inline-start: 40px;

    }


    h1 {
        font-size: 2em;
        font-weight: bold;
    }

    h2 {
        font-size: 1.5em;
        font-weight: bold;

    }

    table {
        margin-top: 4%
    }

    h2, b {
        margin-top: 1%;
        margin-bottom: 1.5%
    }

    b {
        line-height: 5vh;
    }

    .tb_button {
        padding: 1px;
        cursor: pointer;
        border-right: 1px solid #8b8b8b;
        border-left: 1px solid #FFF;
        border-bottom: 1px solid #fff;
    }

    .tb_button.hover {
        border: 2px outset #def;
        background-color: #f8f8f8 !important;
    }

    .ws_toolbar {
        z-index: 100000
    }

    .ws_toolbar .ws_tb_btn {
        cursor: pointer;
        border: 1px solid #555;
        padding: 3px
    }

    .tb_highlight {
        background-color: yellow
    }

    .tb_hide {
        visibility: hidden
    }

    .ws_toolbar img {
        padding: 2px;
        margin: 0px
    }

    #container {
        display: block;
        overflow: hidden;
        width: 830px;
        margin: 0 auto;
    }


    #container li {

        float: left;
        height: 96px;
        list-style: none outside none;
        margin: 6px;
        position: relative;
        width: 125px;

        -moz-box-shadow: 0 0 5px #000;
        -webkit-box-shadow: 0 0 5px #000;
        box-shadow: 0 0 5px #000;
    }

    #container ul {
        overflow: hidden;
    }

    #container ul.hidden {
        display: none;
    }
</style>
<grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration>
</body>

</html>