<!doctype html>
<html class="no-js">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="description" content="">
    <meta name="keywords" content="">
    <meta name="viewport"
          content="width=device-width, initial-scale=1">

    <!-- Set render engine for 360 browser -->
    <meta name="renderer" content="webkit">

    <!-- No Baidu Siteapp-->
    <meta http-equiv="Cache-Control" content="no-siteapp"/>

    <link rel="icon" type="image/png" href="assets/i/favicon.png">

    <!-- Add to homescreen for Chrome on Android -->
    <meta name="mobile-web-app-capable" content="yes">
    <link rel="icon" sizes="192x192" href="assets/i/app-icon72x72@2x.png">

    <!-- Add to homescreen for Safari on iOS -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="Amaze UI"/>
    <link rel="apple-touch-icon-precomposed" href="assets/i/app-icon72x72@2x.png">

    <!-- Tile icon for Win8 (144x144 + tile color) -->
    <meta name="msapplication-TileImage" content="assets/i/app-icon72x72@2x.png">
    <meta name="msapplication-TileColor" content="#0e90d2">
    <title>BJTU-ADAM Lab - 北京交通大学ADAM人工智能实验室官网</title>

    <link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/amazeui/2.7.2/css/amazeui.css">
</head>
<body>

<header data-am-widget="header"
        class="am-header am-header-default">
    <div class="am-header-left am-header-nav ">
        <a href="../../index.html">

            <i class="am-header-icon am-icon-home"></i>
        </a>
    </div>

    <h1 class="am-header-title">
        <a href="../../index.html">
            ADAM Lab
        </a>
    </h1>
    <!--    <div class="am-header-right am-header-nav">-->
    <!--        <a href="#right-link">-->

    <!--            <i class="am-header-icon am-icon-bars"></i>-->
    <!--        </a>-->
    <!--    </div>-->
</header>
<section>

    <h1>
            Post-fairness: Cross-task Adversarial Attack-based Instance Debiasing for Deactivating Spurious Correlations
    </h1>


    <br>
    <div>
        <h2><font face="helvetica" style="font-size:24px">Introduction</font></h2>
                <hr style="margin-top:-10px; margin-bottom:13px">
                <font face="helvetica" style="font-size:18px">
                    <p align="justify">
                        Neural networks often learn to make predictions that rely on social attributes like gender and race, which causes the model to be social biased.
                        While previous work tackles this issue with training an unbiased model, but a stronger and common requirement in practice is a more challenging setting where model weights is unchangeable.
                        In this paper, we propose a post-processing debiasing framework, and explore new ideas for achieving algorithm fairness.
                        Our observations show that the biased model learn the bias feature and make biased decisions based on it.
                        We propose an instance debiasing method based on adversarial attacks to remove the information in the test sample that will activate the bias feature of the model,
                        and require that the features of the new instance deactivate model's biased decision-making relus, so that the model only makes predictions based on the target feature.
                        We demonstrate the effectiveness of this method in laboratory scenarios and real applications.

                    </p>

                    <p>
                        <b>We summarize our main contributions as follows:</b>
                    </p>
                    <ol style="margin-top:-7px">

                        <li>
                            We are the first to propose debiasing through instance transformation. Compared with pre-processing and intermediate processing,
                            our method does not need to modify the model, as a lightweight plug-in to debias the biased model.
                            Our method can also be compatibly used with pre and in-processing at the same time to enhanced debiasing effect.
                            Compared with other post-processing methods, we can make great use of the target feature-based decision rules in the model to achieve debiasing
                            and ensure accuracy on each instance, instead of transformation the prediction distribution to ensure fairness.

                        </li>
                        <li>
                            Under the framework of instance transformation, we explored a function of adversarial attack:
                            As a tool for sample generation, unbiased instances are generated by transforming the original image to remove bias feature.
                        </li>

                    </ol>

                </font>

                <p style="padding-bottom:1px"></p>


                <h2><font face="helvetica" style="font-size:24px">Motivation</font></h2>
                <hr style="margin-top:-10px; margin-bottom:13px">
                <font face="helvetica" style="font-size:15px">


                    <h3>Not all results are based on task-irrelevant features in the input:</h3>

                    <font face="helvetica" style="font-size:15px">
                        <center>
                            <img src="./Post_fairness/Motivation1.png" height="400">
                            <br> <strong>Fig.1</strong> Model bias v.s. The level difficulty of the bias attribute recognition.
                            Model bias of test samples with different levels in the classification of bias attributes.
                        </center>
                    </font>


                    <font face="helvetica" style="font-size:17px">
                        <p align="justify">
                            The observation is "Not all results are based on task-irrelevant features in the input.""
                            Instead, when the bias attribute of the test sample is difficult to recognition, the unfair target model will output fair results.
                            Therefore, We obtain a direct idea: We can hide the bias information in the sample to make the model output from unfair to fair.
                            Based on our above research, we consider that we can use adversarial attack to manipulate image with the target of removing bias information.

                        </p>
                    </font>


                    <br>
                    <p style="padding-bottom:1px"></p>


                    <h2><font face="helvetica" style="font-size:24px">Regular adversarial attack can’t cross task</font></h2>
                    <hr style="margin-top:-10px; margin-bottom:13px">
                    <font face="helvetica" style="font-size:15px">



                        <center>
                            <img src="./Post_fairness/Motivation2.png" height="400">
                            <br><strong>Fig.2</strong> The debiaisng performance for different target model.
                        </center>
                        <br>
                    </font>

                    <font face="helvetica" style="font-size:17px">
                        <p align="justify">
                            When we use adversarial attack achieve instance debiasing, the debiasing performance is not enough and far from our debiasing goal.
                            But we only use the Natural Unbiased Instance to test, the model bias is much lower than the model bias obtained by using the entire test set and achieve excellent debiasing performance.
                        </p>
                    </font>



                    <h2><font face="helvetica" style="font-size:24px">Analysis Framework</font>
                    </h2>
                    <hr style="margin-top:-10px; margin-bottom:13px">
                    <font face="helvetica" style="font-size:15px">
                        <center>
                            <img src="./Post_fairness/Motivation3.png" height="450">
                            <br>
                            <strong>Fig.3</strong> Take a closer look at the different between adversarial attack and Natural Unbiased Instanse by using manifold space as analysis tool.
                        </center>
                    </font>
                    <br>
                    <font face="helvetica" style="font-size:17px">
                        <p align="justify">
                            Based on PCA manifold space, we can observe the relation between adversarial attack direction and manifold feature direction.
                            We use the mean Cosine Similarity to measured, and observed that the mean Cosine Similarity between “Loss Gradient of I ” and “Loss Gradient of M”  is 0.36.
                            This low Cosine Similarity  means The direction of adversarial attack isn’t feature direction，and this leads to underperform debiaisng.
                        </p>
                    </font>

                    <p style="padding-bottom:1px"></p>


                    <h2><font face="helvetica" style="font-size:24px">Proposed Method</font>
                    </h2>
                    <hr style="margin-top:-10px; margin-bottom:13px">
                    <font face="helvetica" style="font-size:15px">
                        <center>
                            <img src="./Post_fairness/Method.png" height="300">
                            <br>
                            <strong>Fig.4</strong> Illustration of the training process of the Post-fairness model.
                        </center>
                    </font>
                    <p style="padding-bottom:1px"></p>


                    <h2><font face="helvetica" style="font-size:24px">Debiasing Evaluation</font>
                    </h2>
                    <hr style="margin-top:-10px; margin-bottom:13px">

                    <font face="helvetica" style="font-size:17px">
                        <p align="justify">

Real-world application does not allow to modify model for debiasing, the only available debiasing method is the post-processing.
We compare the performance of our algorithm with three typical post-processing debiasing algorithms.
In laboratory scenarios, the debiasing algorithms can access to the data preparation and training stage to retrain model.
To evaluate the effectiveness, we consider several typical pre and in-processing debiasing solutions.
We evaluated the proposed AEDA solution on both simulated and real-world bias scenarios, all experimental
results in evaluation support that Post-fairness successfully obtained the unbiased result in the biased model without modifying the original model.
                        </p>
                    </font>


                    <font face="helvetica" style="font-size:15px">
                        <center>
                            <img src="./Post_fairness/Results.png" height="450">
                            <br>
                        </center>
                    </font>
                    <p style="padding-bottom:1px"></p>
    </div>
</section>
</body>


<!--在这里编写你的代码-->

<script src="https://cdn.bootcdn.net/ajax/libs/jquery/3.6.0/jquery.min.js"></script><!--<![endif]-->
<!--[if lte IE 8 ]>
<script src="https://cdn.bootcdn.net/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="http://cdn.staticfile.org/modernizr/2.8.3/modernizr.js"></script>
<script src="https://cdn.bootcdn.net/ajax/libs/amazeui/2.7.2/js/amazeui.ie8polyfill.min.js"></script>
<![endif]-->
<script src="https://cdn.bootcdn.net/ajax/libs/amazeui/2.7.2/js/amazeui.min.js"></script>
</body>

</html>
<style>
    h1{
        font-weight: bold;
    }
    .title {
        color: black;
        font-weight: bold;
        font-size: 2rem;
    }

    p {
        text-indent: 2em;
    }

    .ahref {
        color: black;
        font-weight: bold;
    }

    section {
        padding: 2rem;
    }

    @keyframes flow {
        from {
            filter: hue-rotate(0deg);
        }
        to {
            filter: hue-rotate(360deg);
        }
    }

    figure {
        padding: 0 !important;
        margin: 0 !important;
    }

    img {
        padding: 0 !important;
        margin: 0 !important;
    }

    .am-header-default {
        background-color: linear-gradient(to right, #f6d365 0%, #fda085 100%);
        animation: flow 12s linear infinite;

    }
</style>