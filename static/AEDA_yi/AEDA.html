<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,minimum-scale=1,maximum-scale=1,user-scalable=no">
    <meta name="format-detection" content="telephone=no,email=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="robots" content="index,follow">
    <meta name="referrer" content="never">
    <title>BJTU-ADAM Lab - 北京交通大学ADAM人工智能实验室官网</title>
    <link href="https://ai.tencent.com/ailab/images/favicon.ico" type="image/ico" rel="shortcut icon">
    <script src="./AEDA_files/segment.js"></script>
    <link href="./AEDA_files/app.b56908a4ef7f7c7df6627f70cc26fc81.css" rel="stylesheet">
    <script type="text/javascript" charset="utf-8" async="" src="./AEDA_files/9.691b19c55c2987dd0cf3.js"></script>
</head>

<body _c_t_common="1" style="zoom: 1;" _c_t_j1="1" data-new-gr-c-s-check-loaded="14.1033.0" data-gr-ext-installed="">
<div class="wrap wrap2 wrap_detial">
    <header class="header whiteBg">
        <div class="inner">
            <div class="lang"><a href="https://adam-bjtu.org"
                                 class="router-link-exact-active router-link-active" id="zh" aria-current="page">中</a>
                <span>|</span> <a href="https://adam-bjtu.org" class="langUnFocus"
                                  id="en">En</a></div>
            <div class="navbar" style="">
                <ul style="height: 100%;">
                    <li class="paperNav"><a href="../../index.html"
                                            class="router-link-exact-active router-link-active"><span>主页</span></a></li>
                    <!-- <li class="paperNav"><a href="adam-bjtu.org/paper/AEDA"
                            class="router-link-exact-active router-link-active"><span>论文</span></a></li>
                    <li class="aboutNav"><a href="/about"
                            class=""><span>关于</span></a></li> -->
                    <li class="lang lang2"><a id="zh" class="">中</a> <span></span> <a id="en"
                                                                                      class="langUnFocus">En</a></li>
                </ul>
            </div>
            <!----> <a href="javascript:;" class="nav_btn"></a>
        </div>
    </header>

    <br><br><br><br><br>
    <table border="0" width="1000px" align="center">
        <tbody>
        <tr>
            <td>

                <center>
                    <h1>
                        <font face="helvetica" style="font-size:87%">
                            Towards Accuracy-Fairness Paradox: 
                            <br>
                            Adversarial Example-based Data Augmentation for Visual Debiasing
                        </font>
                    </h1>
                </center>

                <br>

                <h2><font face="helvetica" style="font-size:24px">Introduction</font></h2>
                <hr style="margin-top:-10px; margin-bottom:13px">
                <font face="helvetica" style="font-size:18px">
                    <p align="justify">

                        Machine learning fairness concerns about the biases towards certain protected or sensitive group of 
                        people when addressing the target tasks. This paper studies the debiasing problem in the context of image classification tasks. 
                        Our data analysis on facial attribute recognition demonstrates (1) the attribution of model bias from imbalanced training data distribution and (2) the potential of adversarial examples in balancing data distribution. 
                        We are thus motivated to employ adversarial example to augment the training data for visual debiasing. Specifically, to ensure the adversarial generalization as well as cross-task transferability, 
                        we propose to couple the operations of target task classifier training, bias task classifier training, and adversarial example generation. 
                        The generated adversarial examples supplement the target task training dataset via balancing the distribution over bias variables in an online fashion. 
                        Results on simulated and real-world debiasing experiments demonstrate the effectiveness of the proposed solution in simultaneously improving model accuracy and fairness. 
                        Preliminary experiment on few-shot learning further shows the potential of adversarial attack-based pseudo sample generation as alternative solution to make up for the training data lackage.
                    


                    
                    </p>


                    <p>
                        <b>We summarize our main contributions as follows:</b>
                    </p>
                    <ol style="margin-top:-7px">
                        <li>
                            We propose to employ adversarial examples to balance training data distribution in the way of data augmentation. Simultaneously improved accuracy and fairness are validated from simulated and real-world debiasing evaluation.
                        </li>
                        <li>
                            We provide an online coupled adversarial example generation mechanism, which ensures both the adversarial generalization and cross-task transferability.
                        </li>
                        <li>
                            We explore the potential of adversarial examples as supplementary samples, which provides alternative perspective of employing adversarial attack and opens up possibility to addressing data lackage issue from new ways. 
                        </li>
                    </ol>
                    <div>
                        <div style="margin-left: 10px; margin-top: 20px; display: inline-block;">
                            For more details, please refer to <a href="https://arxiv.org/pdf/2007.13632.pdf" style="color: red;">paper</a>.
                            
                        </div>
                    </div>
                </font>

                <p style="padding-bottom:1px"></p>


                <h2><font face="helvetica" style="font-size:24px">Motivation</font></h2>
                <hr style="margin-top:-10px; margin-bottom:13px">

                <font face="helvetica" style="font-size:19px">Attribution in imbalanced data distribution:</font>
                <font face="helvetica" style="font-size:17px">
                    <p align="justify">
                        Fig.1 shows the calculated model bias (y-axis) for different facial attributes and their corresponding female training image ratio (x-axis). 
                        It is easy to find the strong correlation between model bias and imbalanced data distribution: for facial attributes with a 
                        larger ratio of female in training set (>0.5 in the x-axis), female images are more easily correctly classified than male images, 
                        and vice versa for male (<0.5 in the x-axis). For example, there are more female training images for facial attribute 
                         ``arched eyebrows'', and the corresponding classifier is observed to derive more correct prediction for female images,
                          while male images with arched eyebrows are likely to be incorrectly predicted. The observation suggests that the 
                          classifier learns the correlation between facial attribute and gender from the imbalanced data, and thus utilizes the 
                          gender bias variable for target variable prediction. It well validates the motivation of the previous debiasing attempts 
                          via pre-processing to balance training data distribution, so that the learned model will not utilize the bias variables 
                          for target task prediction. 
                    
                    </p>
                </font>

                <font face="helvetica" style="font-size:15px">




                    <font face="helvetica" style="font-size:15px">
                        <center>
                            <img src="./AEDA/Motivation.png" height="400">
                            <br> <strong>Fig.1</strong> Model bias v.s. imbalanced data distribution. x-axis denotes the female ratio of total people with certain facial attribute in the training set, and y-axis denotes the model bias over gender in predictions testing set.
                        </center>
                    </font>
                    <br>
                    <p style="padding-bottom:1px"></p>


                    <h2><font face="helvetica" style="font-size:24px">The Potential of Adversarial Example in Balancing</font></h2>
                    <hr style="margin-top:-10px; margin-bottom:13px">

                    <font face="helvetica" style="font-size:17px">
                        <p align="justify">
                            We conducted preliminary experiment to justify the feasibility of adversarial examples in switching gender labels and generalizing to original real samples. 
                            Specifically, we first trained binary gender classifier g_ori with original face images from the CelebA dataset, and then employed 
                            I-FGSM to attack each original image to its adversarial image with opposite gender label. 
                            Denoting the original image set as X_ori and the attacked adversarial image set as X_adv, 
                            we constructed the following two training datasets: (1) Hard switch: original image set X_ori with manually switched gender 
                            labels; (2) ADV switch: adversarial image set X_adv with attacked gender labels.
                            <br>
                            We utilized the above two datasets to train gender classifiers g_switch^hard and g_switch^ADV respectively. Fig.2 (top 2 rows) shows their classification accuracy 
                            on the original image testing set.  It is easy to understand the extremely poor performance of g_switch^hard as the manually switched labels make the image-label 
                            correlation exactly the opposite between the training and testing sets. While by replacing the original images with adversarial attacked images, 
                            gender classification accuracy increases from 5.7% to 55.6%, verifying that adversarial examples contain useful information about the attack class 
                            and have potential to generalize to original real data.
                            <br>
                            Following this spirit, we expect that a more robust bias classifier can generate stronger adversarial examples generalizing well to attack class. 
                            Therefore, we first conducted adversarial training on g_ori to improve its robustness and acquired the robust classifier g_robust, and then employed I-FGSM 
                            to attack this robust classifier g_robust to derive new training set ADV switch (robust) with attacked gender label. The learned gender classifier 
                            from this new training set is denoted as g_switch^ADV (robust), whose classification accuracy is shown in the bottom of Fig.2. The significant increase from 55.6%
                             to 89.1% demonstrates the superior generalization potential of adversarial examples from robust models, which motivates us to design more 
                             robust bias classifiers in generating adversarial examples for data augmentation.

                        </p>
                    </font>


                    <font face="helvetica" style="font-size:15px">
                        <center>
                            <img src="./AEDA/Potential.png" height="350">
                            <br><strong>Fig.2</strong> Gender classification accuracy for different training settings with switched labels.
                        </center>

                    </font>

                    <h2><font face="helvetica" style="font-size:24px">Proposed AEDA Framework</font>
                    </h2>
                    <hr style="margin-top:-10px; margin-bottom:13px">


                    <font face="helvetica" style="font-size:17px">
                        <p align="justify">
                            The Potential of Adversarial Example in balancing inspires us to employ adversarial examples to supplement and balance the data distribution for model debiasing. Specifically, 
                            given target task (e.g., predicting the facial attribute of ``arched eyebrow'' from image) with imbalanced training set over bias 
                            variables (e.g., much more female than male samples with ``arched eyebrow'' annotation), adversarial attack is conducted to alter the bias variable 
                            and construct a balanced training set. To guarantee the adversarial generalization and cross-task transferability, 
                            we propose an online coupled adversarial attack mechanism to iteratively optimize between target task training, 
                            bias task training and adversarial example generation. This can be actually regarded as an debiasing attempt between pre-processing and in-processing, 
                            which favorably combines their both advantages. 

                            <br>

                            Specifically, our method has two main parts, the first part is to train the target task by using the augmented data, and the second part train a bias classifier based on the feature extractor of target classifier. 
                            Then, the bias classifier will generate adversarial pseudo examples to augment the target training set. And the iterate fashion of two parts makes that the bias classifier can generate cross-task adversarial examples,  because of the iterate fashion can keep the bias classifier know what features are depended on by the target task.
                
                            Finally, we also use the adversarial training method to learn a more robust bias classifier, and hope the adversarial examples have stronger generalization and better to balance the training data.
                            <br>
                            Therefore, the data distribution of the target task training is balanced, and the target classifier will not learn the spurious correlation of original data.


                        </p>
                    </font>


                    <font face="helvetica" style="font-size:15px">
                        <center>
                            <img src="./AEDA/Method.png" height="350">
                            <br>
                            <strong>Fig.3</strong> Framework of AEDA_online and AEDA_robust. AEDA_robust contains additional adversarial training module (highlighted with red dashline) to AEDA_online. 
                        </center>
                    </font>
                    <p style="padding-bottom:1px"></p>



                    <h2><font face="helvetica" style="font-size:24px">Debiasing Evaluation</font>
                    </h2>
                    <hr style="margin-top:-10px; margin-bottom:13px">

                    <font face="helvetica" style="font-size:17px">
                        <p align="justify">
                            We evaluated the proposed AEDA solution on both simulated and real-world bias scenarios: 
                            (1) For the simulated bias, we used C-MNIST dataset. Its 10 handwritten digit classes (0-9) were regarded as the target task variables, and the associated background colors were regarded as the bias variable. 
                            When classifying digit classes, the model may rely on the background color of image.
                            The goal is to remove the background color bias in digit recognition. (2) For the real-world bias, we used the face dataset CelebA with multiple facial attributes as the target task variables. 
                            The goal is to remove the model’s gender bias in facial attribute classification. 
                        </p>
                    </font>


                    <font face="helvetica" style="font-size:15px">
                        <center>
                            <img src="./AEDA/Results.png" height="350">
                            <br>
                        </center>
                    </font>
                    <p style="padding-bottom:1px"></p>










                    
                    <h2><font face="helvetica" style="font-size:24px">Dataset and Code</font></h2>
                    <hr style="margin-top:-10px; margin-bottom:13px">


                    <font face="helvetica" style="font-size:15px">

                        <div>
                            <a href="" style="text-decoration: none;">
                                <img src="./AEDA/download_button.jpg" height="30px">
                            </a>
                            <div style="margin-left: 10px; margin-top: 20px; display: inline-block;">
                                <a href="https://drive.google.com/file/d/1xifRitx5pTgCKfZWedqD3QmQxeG_9ETo/view?usp=sharing">Dataset and Code
                                    (Google Drive)</a> <br>

                            </div>
                        </div>

                        <br>
                    </font></font></td>
        </tr>
        </tbody>
    </table>

    <div class="clear"></div>
    <div class="footer">
        <div class="footer-box">
            <!-- <div class="left"><img
                    src="data:image/png;base64,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"
                    alt=""></div> -->
            <p>Copyright © 2021 ADaM. All Rights Reserved.</p>
            <ul class="right">
                <!-- <li class="footer_wechat"><a href="javascript:void(0);"><img
                            src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAkCAMAAADFCSheAAAAeFBMVEUAAABfY2hfZGlgZGpgY2lgY2hhZWtjam1gY2lgZGlgZGlkZmt2dopgY2hgZGlfZGlkZWlsbG9gY2lfY2hgZGhfZGlfZGhkaG9gZGhgZGhhZGlgZWlhZWpgZGlhZGhhZGtmd3dgZGlgZGhgY2lgZGlhamxgY2lfY2j4pgKsAAAAJ3RSTlMA+vVI4s84GYl0WSME13lkKgzryr2gjxOYfl9UMJJpUAfFrqSEHrTAcFJPAAABdElEQVQ4y42T6baCMAyE05Z9EWQXRHHN+7/hBWIVWqr3+zWnzAlpJ4EPblvmFueWXSYhfEcUuKDoIjBytFHBPoIBn6EGO8AmDW5SbrVSoYEKNM5oJACFvUUf7rXzUpfwcSWV7RXzgc7jUSYkU4AaiZ1ijuk4H2VA8gEwMJIWrHDwRQe1/X4FT56msKRFicWlyrK5scZvhau3rJPt3L3YVWV/CBb+fsvK/UEU7J18MsikUcdO03z9nzOZO92b7xOunjVz9KHmjesT6txgolCnzRG4hQeUxIqe8s9EeKXwUxoDNg14lK/NoS8LubNwAKit59z16jYXoBwbgOMszjILVs8Pwpatua/W721MwSY+Xw6sWNQ+OWhAjqDzcXfmXeiB+Cx38rPyMFVmsucfSybG3EQUBl75rMA2eFlNZo/7EUh8g/kKxMNVNliHEtQRhuttsz11JvR5rsCMsikYw1doByU8gu9M231r7kHoegwd+Ddp0cLIH4duiLZID23xAAAAAElFTkSuQmCC"></a>
                </li>
                <li class="footer_email"><a href="mailto:ailab@tencent.com"><img
                            src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAgCAMAAABemGpIAAAAdVBMVEUAAABgZGhqc4Bobm5fZGlgY2lfY2hhZWpjZ2xmaW5fY2hgY2lgZGlgZGlkZHNgZGlgZGlgZGlgY2hgZGlgZGlgZGlhZWtgY2lfZGhgY2lgZGliZ21jZWthaGtfZGhfZGlhZGhfZGlhY2lhZWhhZWpgZGlfY2gi7z+FAAAAJnRSTlMA+gcM8+vLQx0V3NeXhxDBvK2gj2JTNHnjs3EvKSbh0WlmXkw6erPR6wkAAAFwSURBVDjLhZKJsoIwDEVTbBFLURBUVveX///Ed1t3KHBmGAZ6oE1uCCRazKITenBXPIu6E4hWuOI5N46IVhEZ3RKt02k3XRO1R0Oa5ZIoMFOuCYiWkjUJZnEmolqMqaLG8tl6kEGOx5v0u/KGxZyBk0G1IGpKn1s2RIuKPzI42XLDoRvaZp34V2a1xx+yvpthx73ivsxyZ8P8dW1sO8kDGRywVHw9iwIvDsxembe2n+qdsO3/lv0y2CCpTrNDd0h2w+OyWNLLwHfYRkzIVwLY+3EicB2Xa/w2dFW5WkP0rR6TE/x0Y2+OBEcJcPPLBm71SOKVTwXb+OSMiLbPjJsmfDYTL7OhnAZ2tgbk2C7ty7aYP/aAUhfhrxxjuC7s5YJxjL/lY4SBEX5ZYMCi40dWXT+sfqydeslyj6GVY+5nHbL7slU8gWrtzpC1O1PJk5SuJk2mcNVO47pVGApcH2dBDlBdQvO4hMEqj8UscY6D/AMQB5GcWnbVDgAAAABJRU5ErkJggg=="></a>
                </li> -->
            </ul>
        </div>
    </div>
</div>
<script type="text/javascript" src="./AEDA_files/manifest.a046e764a7f89ee70324.js"></script>
<script type="text/javascript" src="./AEDA_files/vendor.bf6c914e1185a8f804a7.js"></script>
<script type="text/javascript" src="./AEDA_files/app.fd4ed8c6867ac0aa97f6.js"></script>
<style>
    ol {
        counter-reset: sectioncounter;
        display: block;
        list-style-type: decimal;
        margin-block-start: 1em;
        margin-block-end: 1em;
        margin-inline-start: 0;
        margin-inline-end: 0;
        padding-inline-start: 40px;

    }


    h1 {
        font-size: 2em;
        font-weight: bold;
    }

    h2 {
        font-size: 1.5em;
        font-weight: bold;

    }

    table {
        margin-top: 4%
    }

    h2, b {
        margin-top: 1%;
        margin-bottom: 1.5%
    }

    b {
        line-height: 5vh;
    }

    .tb_button {
        padding: 1px;
        cursor: pointer;
        border-right: 1px solid #8b8b8b;
        border-left: 1px solid #FFF;
        border-bottom: 1px solid #fff;
    }

    .tb_button.hover {
        borer: 2px outset #def;
        background-color: #f8f8f8 !important;
    }

    .ws_toolbar {
        z-index: 100000
    }

    .ws_toolbar .ws_tb_btn {
        cursor: pointer;
        border: 1px solid #555;
        padding: 3px
    }

    .tb_highlight {
        background-color: yellow
    }

    .tb_hide {
        visibility: hidden
    }

    .ws_toolbar img {
        padding: 2px;
        margin: 0px
    }

    #container {
        display: block;
        overflow: hidden;
        width: 830px;
        margin: 0 auto;
    }


    #container li {

        float: left;
        height: 96px;
        list-style: none outside none;
        margin: 6px;
        position: relative;
        width: 125px;

        -moz-box-shadow: 0 0 5px #000;
        -webkit-box-shadow: 0 0 5px #000;
        box-shadow: 0 0 5px #000;
    }

    #container ul {
        overflow: hidden;
    }

    #container ul.hidden {
        display: none;
    }
</style>
<grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration>
</body>

</html>