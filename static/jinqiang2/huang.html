<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,minimum-scale=1,maximum-scale=1,user-scalable=no">
    <meta name="format-detection" content="telephone=no,email=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="robots" content="index,follow">
    <meta name="referrer" content="never">
    <title>BJTU-ADAM Lab - 北京交通大学ADAM人工智能实验室官网</title>
    <link href="https://ai.tencent.com/ailab/images/favicon.ico" type="image/ico" rel="shortcut icon">
    <script src="./huang_files/segment.js"></script>
    <link href="./huang_files/app.b56908a4ef7f7c7df6627f70cc26fc81.css" rel="stylesheet">
    <script type="text/javascript" charset="utf-8" async="" src="./huang_files/9.691b19c55c2987dd0cf3.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
</head>


<body _c_t_common="1" style="zoom: 1;" _c_t_j1="1" data-new-gr-c-s-check-loaded="14.1033.0" data-gr-ext-installed="">
<div class="wrap wrap2 wrap_detial">
    <header class="header whiteBg">
        <div class="inner">
            <div class="lang"><a href="https://adam-bjtu.org"
                                 class="router-link-exact-active router-link-active" id="zh" aria-current="page">中</a>
                <span>|</span> <a href="https://adam-bjtu.org" class="langUnFocus"
                                  id="en">En</a></div>
            <div class="navbar" style="">
                <ul style="height: 100%;">
                    <li class="paperNav"><a href="../../index.html"
                                            class="router-link-exact-active router-link-active"><span>主页</span></a></li>
                    <!-- <li class="paperNav"><a href="adam-bjtu.org/paper/huang"
                            class="router-link-exact-active router-link-active"><span>论文</span></a></li>
                    <li class="aboutNav"><a href="/about"
                            class=""><span>关于</span></a></li> -->
                    <li class="lang lang2"><a id="zh" class="">中</a> <span></span> <a id="en"
                                                                                      class="langUnFocus">En</a></li>
                </ul>
            </div>
            <!----> <a href="javascript:;" class="nav_btn"></a>
        </div>
    </header>

    <br><br><br><br><br>
    <table border="0" width="1000px" align="center">
        <tbody>
        <tr>
            <td>

                <center>
                    <h1>
                        <font face="helvetica" style="font-size:87%">
                            Counterexample Contrastive Learning for Spurious Correlation Elimination
                            <br>
                            基于反例对比学习的虚假相关性消除
                        </font>
                    </h1>
                </center>

                <br>

                <h2><font face="helvetica" style="font-size:24px">概述</font></h2>
                <hr style="margin-top:-10px; margin-bottom:13px">
                <font face="helvetica" style="font-size:18px">
                    <p align="justify" style="text-indent:2em;">
                        机器学习模型在计算机视觉和自然语言处理领域取得了显著成绩。然而，许多工作表明，观测的数据分布中存在的偏见将导致模型学习与标签高度相关的偏见特征[1]。这种学习到的虚假相关性会使模型失败，尤其是在测试数据与训练数据分布不一致的时候。以金发识别的二分类任务为例（如图1（a）所示），如果观测的数据集中的女性图像大多为金发，而男性图像大多为非金发，则模型倾向于学习性别和头发颜色之间的虚假相关性，从而将性别作为预测发色的判别特征。在这种情况下，如果测试集包含显着数量的金发男性图像或非金发女性图像，则模型性能将显着下降。
                    </p>

                    <font face="helvetica" style="font-size:15px">
                        <center>
                            <img src="./CSAN/fig1_v3.bmp" height="500">
                            <br> <strong>图 1</strong> 虚假相关性问题、反事实数据增强去偏方法和本工作去偏方法的示意图
                        </center>
                    </font>
                    
                    <p align="justify" style="text-indent:2em;">
                        为了消除虚假相关性，一种常见且有效的方法是反事实数据增强。通过通常掩盖或替换原始图像中的关键对象生成新样本，以削弱偏见特征和任务标签之间的相关性。图1（b）展示了这样一个例子：通过增加金发男性和非金发女性图像的反事实样本，男性和女性图像在每个任务子集（金发/非金发）中的数量都更加平衡了。这使得模型避免学习了虚假相关性，并更多地依赖任务特征。沿着这条研究路线取得了实质性进展，然而生成反事实样本时具有以下实际问题：（1）高人工标注成本和时间成本； (2) 模型性能对生成样本的质量和真实性高度敏感。    
                    </p>

                    <p align="justify" style="text-indent:2em;">
                        除了求助于额外生成的样本之外，我们认为原始数据集中实际上存在有价值的样本，其消除虚假相关性的潜力没有得到很好的探索。我们将那些与大多数样本具有不一致的偏见-标签对应关系的观测样本称为反例。例如，错误分类的金发男性图像是大多数样本金发女性和非金发男性图像的反例。反例具有和反事实样本相似的特点。这促使我们探索如何利用原始数据集中的这些反例来消除虚假相关。
                    </p>

                    <p>
                        <b>我们的主要贡献总结如下:</b>
                    </p>
                    <ol style="margin-top:-7px">
                        <li>
                            我们引入反例这一新概念来讨论在不引入额外数据的情况下对模型进行去偏的可能性。反例分析研究了反例何时以及如何帮助模型规避虚假相关性。
                        </li>
                        <li>
                            我们提出了使用 CounterCL 来利用有限的观测到的反例来约束特征表示。它与其他解决方案兼容，并且可以作为基于反事实数据增强的解决方案的补充。定量和定性实验验证了有效性。
                        </li>
                    </ol>

                </font>

                <p style="padding-bottom:1px"></p>


                <h2><font face="helvetica" style="font-size:24px">分析</font></h2>
                <hr style="margin-top:-10px; margin-bottom:13px">
                <font face="helvetica" style="font-size:18px">
                    
                    <p align="justify" style="text-indent:2em;">
                        为了研究反例对消除虚假相关性的潜力并启发我们的解决方案，我们对反例进行分析以回答两个问题：（1）反例何时有可能帮助模型避免虚假相关性？ 以及（2）反例如何帮助模型避免虚假相关性？
                    </p>

                    <p>
                        <b>反例的定义</b>
                    </p>
                    
                    <p align="justify" style="text-indent:2em;">
                        已知数据集`{(x_i,y_i)}`，我们将x_i分解为`(t_i,b_i)`，其中`t_i`表示任务特征，`b_i`表示偏见特征。则反例定义如下：如果`y_i=y_j,b_i≠b_j`，则`(x_i,y_i)`是`(x_j,y_j)`的正反例；如果`y_i≠y_j,b_i=b_j`，则`(x_i,y_i)`是`(x_j,y_j)`的负反例。可以看出，`(x_i,y_i)`和`(x_j,y_j)`其实互为对方的反例。为了便于表达，下文中的反例特指数据中偏见-标签对应关系占少数的样本。如图1中，我们将占少数的非金发女性称为反例，且它既是非金发男性的正反例又是金发女性的负反例。
                    </p>

                    <p>
                        <b>反例何时有可能帮助模型避免虚假相关性？</b>
                    </p>

                    <p align="justify" style="text-indent:2em;">
                        为了了解反例在消除虚假相关性方面的潜力，我们首先讨论反例如何在人类做判断时发挥作用。如图2（a）所示，仅给定两类样本进行分类而没有告知类标签的具体含义，很难确定任务是判断性别还是判断发色。但是，如果训练样本中也包含很少的反例（如图 2（b）所示），人类很容易确定分类目标并使用与类别真正相关的任务特征判断类别。这里的反例实际上对帮助人类理解分类任务旨在区分什么提供了重要的帮助。在这种情况下，即使测试数据偏离了训练分布，人做判断时仍然可以可靠地依赖于任务特征，而不会依赖于虚假相关性。
                    </p>

                    
                    <font face="helvetica" style="font-size:15px">
                        <center>
                            
                            <table>
                            <tr>
                                <td><img src="./CSAN/human_a.bmp" height="200"></td>
                                <td><img src="./CSAN/human_b.bmp" height="200"></td>
                            </tr>
                            <tr>
                                <td><center>(a) 没有反例的数据集</center></td>
                                <td><center>(b) 含有反例的数据集</center></td>
                            </tr>
                            </table>
                            <br> <strong>图 2</strong> 人类根据少量反例调整决策的示意图
                        </center>
                    </font>

                    <p align="justify" style="text-indent:2em;">
                        而对于模型来说，上面的例子表明，认识到并强调反例在模型学习中的作用有可能解决去偏问题。所以，我们设计了一个实验来分析在什么情况下反例可以帮助模型避免虚假相关性。为了反映模型依赖虚假相关的程度，我们对不同级别的不平衡训练集采用不同的实验设置，并构建了一个与训练集中大多数样本偏见-任务对应关系不一致的测试集。具体来说，我们在CelebA上进行了实验，并基于标准的训练方法训练了头发颜色为任务目标、性别为偏见特征的二分类模型。如图3所示，在训练集中，金发女性图像和非金发男性图像时占多数的偏见-任务对应关系的样本，金发男性图像和非金发女性图像时占少数的偏见-任务对应关系的样本，即反例。测试集由金发男性图像和非金发女性图像组成，它们的偏见-任务对应关系与虚假相关性冲突（相反）。进一步，为了实验结果的可比性，我们保证训练样本总数不变，逐步增加反例在训练集中的比例ρ。准确率如图4所示。我们可以看到，随着反例比例的增加，准确率不断提高。实验结果表明，基于标准训练方法，当反例比例较低时，反例对虚假相关性消除的贡献有限；而随着其比例的增加，反例逐渐帮助模型避免虚假相关性。
                    </p>
                    
                    
                    <font face="helvetica" style="font-size:15px">
                        <center>
                            <img src="./CSAN/fig.3a.bmp" height="300">
                            <br> <strong>图 3</strong> 实验设置
                        </center>
                    </font>

                    <font face="helvetica" style="font-size:15px">
                        <center>
                            <img src="./CSAN/fig4.png" height="300">
                            <br> <strong>图 4</strong> 不同反例比例实验的准确率。不同颜色的柱子的长度表示不同样本的比例。
                        </center>
                    </font>

                    <p>
                        <b>反例如何帮助模型避免虚假相关性？</b>
                    </p>

                    <p align="justify" style="text-indent:2em;">
                        上一小节观察到，随着反例比例的增加，反例有助于模型避免虚假相关性。在本小节中，我们将进一步研究这一观察背后的机制。如之前的实验所示，错误分类的金发男性图像表明训练模型倾向于将反例视为与错误类别更相似样本。为了量化反例和正确类样本的相似程度，我们计算反例在特征空间中与正确类中心和错误类中心的相对位置。具体来说，给定类别`c`，我们将Relative Counterexample Affinity (RCA) 定义如下：
                    </p>

                    <p>
                        <center>`RCA(c)=\frac{\frac{1}{N}\sum_{i \in CE(c)}dis(e_{i}\ ,\ e_{\bar{c}})}{\frac{1}{N}\sum_{i \in CE(c)}(dis(e_{i}\ ,\ e_{c}) + dis(e_{i}\ ,\ e_{\bar{c}}))}`</center>
                    </p>

                    <p align="justify">
                        其中`dis(∙,∙)`是欧氏距离的计算函数，`CE(c)`是类别`c`中反例的索引集，`N=|CE(c)|`是反例的数量，`e_i`是反例的嵌入表示，`e_c`和`e_{\bar{c}}`是类别`c`的类中心表示以及平均其他类的类中心表示。较高的 RCA 值表明反例与其所属的类中心之间的相对距离较近，反之亦然。 我们在上述实验中计算了两类的 RCA。非金发女性图像是非金发类的反例，金发男性图像是金发类的反例。如图5所示，无论哪种类型的反例，RCA都随着反例比例的增加而不断增加。这意味着反例通过鼓励其表示接近来自同一类的大多数样本的表示来帮助模型避免虚假相关性。受此启发，在反例数量固定的前提下，我们提出了反例对比学习（CounterCL），它直接调节反例的特征表示来模仿上述反例比例增加的结果。    
                    </p>

                    <font face="helvetica" style="font-size:15px">
                        <center>
                            <img src="./CSAN/curve.png" height="300">
                            <br> <strong>图 5</strong> RCA随反例比例增加的变化趋势
                        </center>
                    </font>

                    <h2><font face="helvetica" style="font-size:24px">反例对比学习</font></h2>
                    <hr style="margin-top:-10px; margin-bottom:13px">

                    <font face="helvetica" style="font-size:15px">
                        <center>
                            <img src="./CSAN/fig5.bmp" height="300">
                            <br> <strong>图 6</strong> 反例对比学习的模型图。在特征空间中，CounterCL损失拉近正反例对并推远负反例对。
                        </center>
                    </font>

                    <p align="justify" style="text-indent:2em;">
                        上述分析实验表明，反例通过鼓励其表示接近同一类的大多数样本的表示，从而有助于消除虚假相关性。受自监督对比学习的启发[2]，我们定义Counterexample Contrastive Learning (CounterCL) 损失，以此调整反例在特征空间的表示。如图6所示，我们以三元组`(x_{o ri}, x_{pos}, x_{n eg})`为例，`x_{pos}`是`x_{o ri}`的正反例，`x_{n eg}`是`x_{o ri}`的负反例。上述样本通过一次随机的变化（旋转或裁剪等）后，通过主干网络得到其特征空间的表示。这些表示可以分别计算分类任务的交叉熵损失和去偏的CounterCL损失。CounterCL损失形式化的定义如下：    
                    </p>

                    <p>
                        <center>`L_{CounterCL}=-\frac{1}{2N}\sum_{i\in\tilde{I}}\frac{1}{|J(i)|}\sum_{j\in J(i)}log\frac{exp({z_i\cdot z_{j}}/\tau)}{\sum_{\alpha\in J(i)\cup K(i)}\ exp({z_i\cdot z_{\alpha}}/\tau)}`</center>
                    </p>


                    <p align="justify">
                        其中，`J(i)`是样本`x_i`的正反例的索引集合，`K(i)`是样本`x_i`的负反例的索引集合，其他部分与自监督对比学习的定义一致。    
                    </p>


                    <h2><font face="helvetica" style="font-size:24px">实验</font></h2>
                    <hr style="margin-top:-10px; margin-bottom:13px">

                    <p align="justify" style="text-indent:2em;">
                        我们在CelebA和UTKFace数据集上验证了方法的有效性。
                    </p>

                    <p>
                        <b>比较结果</b>
                    </p>

                    <font face="helvetica" style="font-size:15px">
                        <center>
                            <strong>表 1</strong> CelebA数据集的实验结果。加粗为最优结果，下划线为次优结果。
                            <br><img src="./CSAN/table1.png" height="300">
                        </center>
                    </font>

                    <font face="helvetica" style="font-size:15px">
                        <center>
                            <strong>表 2</strong> UTKFace数据集的实验结果
                            <br><img src="./CSAN/table2.png" height="200">
                        </center>
                    </font>

                    <p align="justify" style="text-indent:2em;">
                        CelebA 和 UTKFace 的定量实验结果见表1和表2。在 CelebA 上，CounterCL 在所有任务中都达到了最好或次优的准确度。在 UTKFace 上，CounterCL 在两种偏见设置上都明显优于其他方法。这些结果验证了我们的方法在消除虚假相关方面的有效性。
                    </p>

                    <font face="helvetica" style="font-size:15px">
                        <center>
                            <img src="./CSAN/fig_umap_a.png" height="300">
                            <br> （a）Vanilla
                        </center>
                    </font>

                    <font face="helvetica" style="font-size:15px">
                        <center>
                            <img src="./CSAN/fig_umap_b.png" height="300">
                            <br> （b）CounterCL
                            <br> <strong>图 7</strong> 样本在特征空间的可视化
                        </center>
                    </font>

                    <p align="justify" style="text-indent:2em;">
                        在图7中，我们通过 UMAP可视化了CelebA 中 BlondHair 分类任务的样本的表示。不同颜色的点代表具有不同偏见或不同标签的样本。我们可以看到，Vanilla 获得的表示无论是发色还是性别都表现出更强的可分性。头发颜色和性别之间的分布一致性表明模型严重依赖偏见。而 CounterCL 获得的男性和女性图像的表示均匀地分散在特征空间中。头发颜色和性别之间明显的分布差异表明我们的方法使模型没有将偏见特征作为判别特征。
                    </p>

                    <p>
                        <b>兼容性评估</b>
                    </p>

                    <font face="helvetica" style="font-size:15px">
                        <center>
                            <strong>表 3</strong> 在UTKFace数据集上其他去偏方法结合CounterCL的实验结果
                            <br><img src="./CSAN/table3.png" height="150">
                        </center>
                    </font>

                    <p align="justify" style="text-indent:2em;">
                        CounterCL作为正则化项可以灵活地与其他方法结合使用。如表3所示，与原始方法相比，CounterCL可以提高大多数去偏方法的性能。具体来说，结合 CounterCL的LNL 和 EnD 可以显着提高至接近SoTA的性能。值得注意的是，一些结合 CounterCL 的方法可以比 CounterCL 更好地发挥反例的潜力。除了在不引入额外数据的情况下与其他方法结合之外，CounterCL 还可以轻松扩展以与现有的基于数据增强的方法结合。未来，我们正在努力探索融合增强的外部数据和内部反例以消除虚假相关性的可能性。
                    </p>

                    <h2><font face="helvetica" style="font-size:24px">结论</font></h2>
                    <hr style="margin-top:-10px; margin-bottom:13px">

                    <p align="justify" style="text-indent:2em;">
                        在这项工作中，我们引入反例这一概念来讨论在不引入额外数据的情况下对模型进行去偏的可能性。通过分析实验来研究反例何时以及如何帮助模型避免虚假相关性，我们提出 CounterCL 来利用有限的观测到的反例来调整特征表示。它与其他解决方案兼容，并且可以作为基于反事实数据增强的解决方案的补充。定量和定性实验验证了有效性。
                    </p>

                    <p align="justify" style="text-indent:2em;">
                        除了将反例与增强的外部数据整合外，未来我们还将朝着以下两个方向努力：（1）以修改损失以外的方法利用反例，并且去除偏见标注的要求以提高实用性； (2) 探索反例在重训练之外的场景中的应用，例如，使用主动挑选的反例进行在线调试。
                    </p>
                    <br>

                    <hr style="margin-top:-10px; margin-bottom:13px">

                    <p>
                        <b>参考文献</b>
                    </p>

                    <p align="justify">
                        [1] Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. 2018. Don’t just assume; look and answer: Overcoming priors for visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 4971–4980.
                        <br>
                        [2] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In International conference on machine learning. PMLR, 1597–1607.
                    </p>




                    
        </tr>
        </tbody>
    </table>

    <div class="clear"></div>
    <div class="footer">
        <div class="footer-box">
            <!-- <div class="left"><img
                    src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAqAAAAAsCAMAAACND/p9AAAAk1BMVEUAAAA+PkxXW2JeYmdPVFheYmddYGVbXmVdYWZfY2dWW15eYmdeYmdfY2deYmdeYmdeYmdbYGVeYmdcX2VeYmdZXmFeYmdeYmddYmZeYmdeYmheYmdfYmdeYmdeYmdeYmdeYmddYWZcYGVeYmdeYmdeYmZeYWZdYWdeYmdeYmdeYmddYWZcYmZeYmdeYmdfY2isr7IME2X3AAAAL3RSTlMAAxP7Cdc0HTr3D+bc7/Nsoyi1I5MY64NVZKrRy7uMdZ5GLeJ8XExRxsGZPkKwiDQMG50AABB7SURBVHja7NfbbptAGATgWSBgiAGvDxiMAzQ+xOfJ+z9dHQoE17iWqpZgZb8LpNVe8f8jNED5cmK0D/00CnuWgKJ0ixZmscdfvHi3dqAonSGTBS/1pxYUpRMmicuSzVKwVF/RGiPmHSs8IENKOQIikkNAO58sFFYkYwP/yPs7/ta6z8LGN6WZxizEeyglK+Cf6Y8yrcOmcDSBUd91+xqmJDOIged6EQo9koEGmMNr2WtrARVTvZxwKvDBOLFg+1AKe94RPEonWrK0EXCeSN3EjOQSpl1/DSvIr2CywaKtgBoZSylKJ5YS9UNfrfWO+FFGNasHFC95OZnnzx3J8WWpCYHJM6/1WwqoyFjaiqa+lUDJxbxjho4ZOTfLtD0+bYswjkkunYDUpeWSnkRl+HEFiN1L3dbjWdJSQE/NHT9lJcV3NDHNCS68hZ9WC+aysOYVHTPdyMbcBuQc8ElGPd8/8nw8kbTTGcmF70sUEpKDMtao7Hi2biegISu2xKceK/oBHWK8RaGD/y8jd7hJC5j7gS6bcbG+UaYHwIDk4cgG1cIjknOBXBJdVARXthJQ6/lGhXJqF08afifTK5FAG6ROmmhmrP3UPwhUJlG6EriwT30Td+Xry3BTjznvJ7XWuaUoDIWlCMrQi8hIFXQoKu//dptyIcE4xy1nObvfn2FuQow3X271nzOagitvxGNJiFuUWrx/cxSx13kq2/T0pXTMwYhKW0wKYxWCliNDKAbSgIfwnjMKsOTNGgj23xM0ebZqFdLvk6NGJqP/c4ImkCj804XiyCKR8VIBEHR+kcxnG4UlQYY95WdJ8Zg5jYd7SlcZTfeDOUQY/VXqoJoiGnbAhRuxg/+DoP6I0S8IepCFDDb8c4KGI0G2+ZcxgGd8UpeUESN5RkN3PhO5v6UKaO4GRmsFgrojA98ygBvzfSavJxQhIqZDH4/fuPj1CKrbyCaAWfu7BJWy13qRIm9X3TSdV6Uq8xP084DGQVXC24G2G3Zn3RCcNXqtGs6L9wyV/0fXbrdzJJh5AitZnqyJk/gcR5/xkBOE2NZ2jxyA55/hOTa4xQpmnS4rEBSfKEMhpPcMH4J1ZAyo1c2a0BFBvW/97uFLAVfwVwlqQOiWLIRV3n0ohBFpuGOm55RMGtrl7YGMd0UkXLxL39V7SqjTZ+JJ/K94UpqTHdrYY9+2rebQuzxZ5Jj8C/6oo09w37LojT8jjzWzj/34CuVcxmHHmzMni19SdmsQlHfUtfysMYsb/fquSbogr3wM/TZts9IJwCY4YWNsvvosTbv+KnFKcsqsTTu/jC/eZJN7JGn98ALrGU3obNTE7dI0y3dU9AhDF1HgMwzDXH7l4X3pBAT8JYJG14bs57MZJEbQfjPkGfo6ruMJmZrQ25SdRZ1UcXW2h+5G6OkrXOJZLSzk8W49daU0GNIfJi+3YmPuGyguydGOKTfeIZEuZj5ZwJXL2Ca1dHwFh8tB7lz4ZNPv1OG7EK1BUH+xqWfE3Gj/MwT9MpkeafKo1sgXuJPQ16fD7G1OpURUHNhVuVASIAVeu0mVvUpFDEICjavMCTb8W/VXCepz29FmgrpzD9gmeYNwxoeIKeIk9JUC5huV0tsU+8WwfeOWE3liBXDDhSbO3WDFSOsRqU9G0EQv7vZi3+e66Ijd49xCR6cXcdzENcmlcvxS18QIHt52HOc2izWl7VxxMmr8uFmBoOSTAK0qDm/5EtR7gt6IWmzLZhGDivu8REjGWoPysyWcsyx7PwXeziQhfysINkFIHkomonLBz19wNkcKQMOvErSHjXO2y6U7V0DaiF1PphX2cwblY2vaE9UN7p5/+CMPvk4TuaOILVlanob29Rb8/7x9n87rhH6W9Lor26hcfWa3iWza6NwYIV7YVzd3zGNvEe7TWLNqmobQ12wwyPxyDYKeFXaftY2IihvX3xJUQqpSck2PdK05gFjFHK8LJAy+stlM55iryRnN9HbH+AwZmVVgyRAqpOcK6knxNC850fhHGqoqQWKnqirsi4UCUkY/Mv9Vgmr9hWycuOlWmvPHDn28vguVxTLM/7kTP3sI1WPNUI2gsHinxNHv0LlcYHiBU5htr7J146IIW3tqm8rAvpPjoXW1kAUWyx//1b7r18xvo8Gwy7ItcLuj/ckOCtVqinStbyrKMsMxCasRe/UaL3Xkmjiwmj+KKNYg6FVw8N87ee0tQT2FMeOKnisg6AdcSjUDHxEdsEIEl5mwJqutA0FTsDI4qipZkiSWx2FZh9J/q/5ukmRg6g0Tr0qJ6eHGp9spnNIyn/SD6WOpEU1Vem8BdaGTcBOoBF/IMycjWQ4qXf2MfGuMp97BZMrLnMGFXhYgPeJxudg/caeB4am8Dn8ji8gKPFOPKYuVe1+WPX32rwZ3/B8y9qQirmsQNHn3gxDJZRt6S9Aj1yzFqYoDBA3nucC8ipvIVGGqXHl5AIJeOJ96kp7KTKKH3we0Sqfsfo+gQPMCCFrLXLUjESrgc+p4VOjJGlzIgKGcoahD0AXs9iKApZbTiWYD36gyL8w+PNh5tNAe4EvYpfxkTsplDdF90QT7pC2a2+f4CqfbpCrizKKTadK4pTYx8EWwvRUIClcMLIUIeocBx7cELRjJiFeIgaDJcwUzgQiLg8uViVWTnPySiVdg2kIsMK+b3n/8fpnJh3GXHCPLEBr+jH+0dy1bisJA1ADyjCgooiLagrb4/v+/GyGJVVgy6Cw4s/BuuoUQUS71uFVB/dG8ABTMI9LhMIOE/5ZYj9kSJDyvVAJZP82yx90UdjhlsHkFBdaZypnYs+yyqL/Nlkg31baiZFllGx1HFyj/R9bIWAq2M03TovL/PNDusMp5udElQY/sLy7vbYLGFUEgHrxKgu6w3TmLb2jJXrBCgYtjakw8lXFTI0GVy1urKk/CPieoEVkRk/KSkpkU7pcnpq4nd8WdSTXRswycIOHX9ySCLb+EzJdkBXbDMJodrCEI3dayfZmRYgtqTV8ohKq7ztxMyxKTc+P+3WAGFcpYJN4lpSMCP7FGrRkFzBv2OnTxq5Z25nYXrwKxpepI2y9fEXTaSND0bgnUsTvvBUHtVoJChLERof1HBNXmFz71p8nP7wUI+kMIWo85U4Z44c/3ADlgCETiBiHoAkqmzrlHEPjiwNkesJAEhUkuZM2UOtMTmHlSBNMdvTw2m/yOvTtRtYeQtO8Zs1FoiAlQY4MJmfsJwo5ukqQQk4XCWL6ZJK1uGJ8SFONjgkpCOcWhxBIuUitBYVEWoJ2gIdZ7znrzIjrMRglIm64QoIasuYpC4WclnYgDj3Jwds39LG5eu0JmaYUjESGo0JLBCfhj1JkxxIXOrmSm9prVHmSmjwi66pKg0DoA7vN9gm6cqsCYXi6X0GsnqJFLA9m2EsQs729CJNd7fEhbh/CAqmYUUHi2cJ4GVk6mZrifhdGL7idVaXQnadw3gzvK/zw7i2qZXiLpHZNCZ1dCvZJkWnpzE/YOQXmMsPmMoEWMkH1AUPAGvoQjr1I7QcE3enPZiTFqISg439tJHkAB1eNoSmL4TBLJhJYx86NFzinYbScgmVkeoZuefthd5SJYKE5AmBW9hDipW25DLCBizUATd8sgkJvhS+2i1Jm2DWLJe6XOFWnY+oCgJNn8lKA/ZW3REsg8Md27BLVV8aqdoFg/HggmSIfN+8/gE+hsdkxCpKkmkx5MM+Kw9Rfzzl878BjUK9LPArhW2wwumUxF+BU2X9ceGyZueR2VKdM4OKhOmkUOLWNU8PxOkhS+S9CEZvHXDwlKnUGMSViw9wm6EzkMIegI36NbnEtjBVz6cNPVnmGQhL8moENJUjcb1zpwi87LXjvwC0riQR94dsZp9XZjWeg0whr1ZeAChn5vLW6pVLBCiAvFN9NFux3wqxFjMaiVoEMhb7YTdEbn+wHr20pQnxBUZu52LaPP/o2gLAWCHnCAOyddtAvsSml1qxaZH17XSee3xhg04o09+9iB05hgWDPzU4vG6lzrbRTLGIJb8dBGuYcei8LxBjL3GUTUnTQstydka2H2Wwlqi7naCRqU1XRiyn2zlaBYTaXdHolBykFvEvQX2G0cIYsHv7wDvYUq4MemYrFbjsUJPz3c1pUxJoeyouFpJcxAufictkeeSaETIGLXi/CdQ2M+xNj0TB0YDcnfvJa5xyA6drTkg0d/HRJMQWxoICjoUf5QvRhHjQRlKRroBq7slMtnLmxqIKicMhwbjNUuKFSPwMc3E5QhqKp/UL1NimWmW2Kzqno+BRmDKOAq0ek/mapsUWWJXNGXHP6LG8YmrHa/7pQTpwrUORy/duBnpNxjeYuam7W4qzYsv9XVtTkwGpK/PBN1UceU8yolv7NFc4V6OyP4HcaTySoebsZMfc8JlWUpQUGs8dLRaHE59P2wkaA9+zEwLLg+fIhrPBSbnGsjQeWF8/v9ZPxUubGf6vJBI0EHSR9wkmZusLhOitKCIILe9OVC9h/94EIn6veDx24UyAUwezLQpRSBV25jAX1cM1ML+Djm4nZC1nWEmG9sFnpuoIZUi+imUpVhSN7CuAijXFRJ+eGpTXmFGM0S6IBhxaNF2Ugg7uhs2bG3zVx3vBv1HdCCk+PMMlwzHsCy49aG5RnWIp1IGBBc6lSOcu/dSMOy//wMmUDHBAUbNuY0+j/KC4Pfai0IOogIQYnkmkGv2lpk76JZBM4p1chyQq6BUaow+NmPXcbcYBMXuryKG9qtOMftoFr/0Yo32tlBYG7WqSPOmakz9VZny2BMy2aTpGRMgwP/kYRCF5Y6Yy2v1C/Nq8Lu+Qgj6IXAaCm0+mtDKQqF+mYhX+juwQ1Ozn3a1M1zve3BDSfuHVyg+7aYii7f/DARIZgHrdxa4XHzMXAgBvJUer9sJdbVOPdNW1aOSbx+8HCY3FsqplnxgXteHiCDtfC8uqk4el4l8nBPNEnjPXWcqykn1c05vZx7Vy+PK1Eiz7d2OKiakNO5QfIeSKCNFL6yvM/zgdxPzBp4ACDSSYyG3mzgHVq5MeV97vlIBho11qfihoX7YL4vQtMPX6+02mNDv7RhRecRCp3h737/63b/6BuK9t5UwzXqyWdm22bwWJfIxG46lomBEdrNrPomgx4Lr0qxBcBc9YawgcFhGC7RbGTQbJ7MCI6Uf6371kBrSaCjcgvBCmVQ2OQc6nLjxiNHKhUn67+ad44ceHuhU6P9QUNhlq4oQloK6EL9UBPoExesGBQ6BRK304eHUXwfHvb+ik73SvyPvwiggROTQXvOv4OF3mQSrBHZNZ1E4MCxPJ0hSbV54X4qkp0YFBcwt8BoMQF/zH6FtpEjnGOXj1+k+D5+keIvCni2wtbOO6wzKBPXyZA5pLp5HuWI2f3RTgOX/IMz7TycW4JRtAK1e6fQaXl6Jcddbrpj42q2LgGMPlwslFnpulAHWCoHTse9zh9gi/F9gG0jXFvAeN68DfnAnyaX7flxaQ051m05PNqsCs8f8DD+DeQO2HU95L7vLUdDO3ow6lROcTLRWEtuktc0Ey+tp5M0TbskdWab+Fh2fy1gj9VI/KitajfDI+2s+0eAY3wfAf7Ff4VohZLJ748ofPH/4fszNF/856j/kNfk+0NeX/x3+P4U4l/wB2FPfKWU9dsLAAAAAElFTkSuQmCC"
                    alt=""></div> -->
            <p>Copyright © 2021 ADaM. All Rights Reserved.</p>
            <ul class="right">
                <!-- <li class="footer_wechat"><a href="javascript:void(0);"><img
                            src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAkCAMAAADFCSheAAAAeFBMVEUAAABfY2hfZGlgZGpgY2lgY2hhZWtjam1gY2lgZGlgZGlkZmt2dopgY2hgZGlfZGlkZWlsbG9gY2lfY2hgZGhfZGlfZGhkaG9gZGhgZGhhZGlgZWlhZWpgZGlhZGhhZGtmd3dgZGlgZGhgY2lgZGlhamxgY2lfY2j4pgKsAAAAJ3RSTlMA+vVI4s84GYl0WSME13lkKgzryr2gjxOYfl9UMJJpUAfFrqSEHrTAcFJPAAABdElEQVQ4y42T6baCMAyE05Z9EWQXRHHN+7/hBWIVWqr3+zWnzAlpJ4EPblvmFueWXSYhfEcUuKDoIjBytFHBPoIBn6EGO8AmDW5SbrVSoYEKNM5oJACFvUUf7rXzUpfwcSWV7RXzgc7jUSYkU4AaiZ1ijuk4H2VA8gEwMJIWrHDwRQe1/X4FT56msKRFicWlyrK5scZvhau3rJPt3L3YVWV/CBb+fsvK/UEU7J18MsikUcdO03z9nzOZO92b7xOunjVz9KHmjesT6txgolCnzRG4hQeUxIqe8s9EeKXwUxoDNg14lK/NoS8LubNwAKit59z16jYXoBwbgOMszjILVs8Pwpatua/W721MwSY+Xw6sWNQ+OWhAjqDzcXfmXeiB+Cx38rPyMFVmsucfSybG3EQUBl75rMA2eFlNZo/7EUh8g/kKxMNVNliHEtQRhuttsz11JvR5rsCMsikYw1doByU8gu9M231r7kHoegwd+Ddp0cLIH4duiLZID23xAAAAAElFTkSuQmCC"></a>
                </li>
                <li class="footer_email"><a href="mailto:ailab@tencent.com"><img
                            src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAgCAMAAABemGpIAAAAdVBMVEUAAABgZGhqc4Bobm5fZGlgY2lfY2hhZWpjZ2xmaW5fY2hgY2lgZGlgZGlkZHNgZGlgZGlgZGlgY2hgZGlgZGlgZGlhZWtgY2lfZGhgY2lgZGliZ21jZWthaGtfZGhfZGlhZGhfZGlhY2lhZWhhZWpgZGlfY2gi7z+FAAAAJnRSTlMA+gcM8+vLQx0V3NeXhxDBvK2gj2JTNHnjs3EvKSbh0WlmXkw6erPR6wkAAAFwSURBVDjLhZKJsoIwDEVTbBFLURBUVveX///Ed1t3KHBmGAZ6oE1uCCRazKITenBXPIu6E4hWuOI5N46IVhEZ3RKt02k3XRO1R0Oa5ZIoMFOuCYiWkjUJZnEmolqMqaLG8tl6kEGOx5v0u/KGxZyBk0G1IGpKn1s2RIuKPzI42XLDoRvaZp34V2a1xx+yvpthx73ivsxyZ8P8dW1sO8kDGRywVHw9iwIvDsxembe2n+qdsO3/lv0y2CCpTrNDd0h2w+OyWNLLwHfYRkzIVwLY+3EicB2Xa/w2dFW5WkP0rR6TE/x0Y2+OBEcJcPPLBm71SOKVTwXb+OSMiLbPjJsmfDYTL7OhnAZ2tgbk2C7ty7aYP/aAUhfhrxxjuC7s5YJxjL/lY4SBEX5ZYMCi40dWXT+sfqydeslyj6GVY+5nHbL7slU8gWrtzpC1O1PJk5SuJk2mcNVO47pVGApcH2dBDlBdQvO4hMEqj8UscY6D/AMQB5GcWnbVDgAAAABJRU5ErkJggg=="></a>
                </li> -->
            </ul>
        </div>
    </div>
</div>
<script type="text/javascript" src="./huang_files/manifest.a046e764a7f89ee70324.js"></script>
<script type="text/javascript" src="./huang_files/vendor.bf6c914e1185a8f804a7.js"></script>
<script type="text/javascript" src="./huang_files/app.fd4ed8c6867ac0aa97f6.js"></script>
<style>
    ol {
        counter-reset: sectioncounter;
        display: block;
        list-style-type: decimal;
        margin-block-start: 1em;
        margin-block-end: 1em;
        margin-inline-start: 0;
        margin-inline-end: 0;
        padding-inline-start: 40px;

    }


    h1 {
        font-size: 2em;
        font-weight: bold;
    }

    h2 {
        font-size: 1.5em;
        font-weight: bold;

    }

    table {
        margin-top: 4%
    }

    h2, b {
        margin-top: 1%;
        margin-bottom: 1.5%
    }

    b {
        line-height: 5vh;
    }

    .tb_button {
        padding: 1px;
        cursor: pointer;
        border-right: 1px solid #8b8b8b;
        border-left: 1px solid #FFF;
        border-bottom: 1px solid #fff;
    }

    .tb_button.hover {
        borer: 2px outset #def;
        background-color: #f8f8f8 !important;
    }

    .ws_toolbar {
        z-index: 100000
    }

    .ws_toolbar .ws_tb_btn {
        cursor: pointer;
        border: 1px solid #555;
        padding: 3px
    }

    .tb_highlight {
        background-color: yellow
    }

    .tb_hide {
        visibility: hidden
    }

    .ws_toolbar img {
        padding: 2px;
        margin: 0px
    }

    #container {
        display: block;
        overflow: hidden;
        width: 830px;
        margin: 0 auto;
    }


    #container li {

        float: left;
        height: 96px;
        list-style: none outside none;
        margin: 6px;
        position: relative;
        width: 125px;

        -moz-box-shadow: 0 0 5px #000;
        -webkit-box-shadow: 0 0 5px #000;
        box-shadow: 0 0 5px #000;
    }

    #container ul {
        overflow: hidden;
    }

    #container ul.hidden {
        display: none;
    }
</style>
<grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration>
</body>

</html>